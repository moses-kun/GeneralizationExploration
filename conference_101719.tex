\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Exploring Generalization of Seq2seq model for Fog Computing Application Placement Problem \\
% should not be used
\thanks{Identify applicable funding agency here. If none, delete this}}
\makeatletter % changes the catcode of @ to 11
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother % changes the catcode of @ back to 12

\author{
\IEEEauthorblockN{Michael S. M. Pakpahan}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
msmpakpahan@mail.ugm.ac.id}
\and
\IEEEauthorblockN{Lukito Edi Nugroho}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
lukito@ugm.ac.id}
\and
\IEEEauthorblockN{Widyawan}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
widyawan@ugm.ac.id}
\linebreakand

\IEEEauthorblockN{Ajie Kusuma Wardhana}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
ajie.kusuma.wardhana@mail.ugm.ac.id}
\and
\IEEEauthorblockN{Muhammad Ardian R.A}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
muhardian.ardian@mail.ugm.ac.id}
\and
\IEEEauthorblockN{Rangga Satria Astagenta}
\IEEEauthorblockA{\textit{Dept. of Electrical and} \\ 
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia \\
ranggastariaa@mail.ugm.ac.id}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.

In this paper we discuss the use of multihead attention model to improve performance of fog application placement model. We hypothese that using the multihead attention model, we can increase consideration of every input varible. Resulting a more aware model, that consider not only the applicaiton but also where it is deployed. We compare base model with single layer attention, 16 and 32 head model attention, and heuristic algorithms. 
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
Fog computing leverage the the abundant devices on the network to do computation. Extending compuation from a central cloud, into networks. In theory, the closer the computation of application, the lower the network usage, the lower latency of application, but still infinite resource of cloud are still available to use. However, this concept require optimization of application placement. Where application needed to be placed on the network for improvement.


However, efficiently placing applications in the fog network, particularly in an unseen environment, remains a formidable challenge. The application placement problem is intrinsically complex, primarily due to its multi-objective nature, requiring the optimization of a variety of parameters such as latency, bandwidth, and resource usage. Additionally, the fog environment is inherently dynamic and heterogeneous, with changes in network conditions, resource availability, and application demands. Therefore, an effective placement algorithm needs to be adaptable and generalizable to unseen situations.

This research explores the generalization capabilities of deep learning models for the fog application placement problem. Through a rigorous analysis of average response time, byte size, and hop count across various methods, we assess the performance of these models in unseen scenarios. The results indicate that certain deep learning methods outperform others, shedding light on their potential applicability for the fog application placement problem.

The relevance of this research extends beyond academic interest, having considerable implications for the future resilience of networks. A robust placement algorithm, with strong generalization capabilities, can maintain network performance and service quality despite disruptions and uncertainties. By contributing to the development of such algorithms, this research advances efforts to create more resilient and efficient fog networks, paving the way for a future equipped to handle the ever-evolving challenges of digital connectivity.

In the context of Seq2Seq models, which are widely used for sequence generation tasks, achieving good generalization is essential to ensure their practical applicability and usefulness in real-world scenarios.

\section{Related Research}

The increasing adoption of fog computing has opened up new possibilities for the efficient deployment of applications and services at the network edge, closer to the end-users and data sources. However, fog computing introduces unique challenges in terms of application placement due to the distributed and heterogeneous nature of fog nodes, varying resource capabilities, and dynamic network conditions. To address these challenges, there is a growing need for intelligent and adaptive application placement strategies that can optimize resource utilization, reduce latency, and enhance overall system performance in fog environments.

Traditional approaches to application placement in fog computing often rely on heuristic-based methods or simple rule-based algorithms that do not fully leverage the power of data-driven decision-making. These conventional approaches may struggle to cope with the dynamic and diverse nature of fog environments, leading to suboptimal resource allocation, increased latency, and reduced application performance.

In this context, the Sequence-to-Sequence (Seq2Seq) method, which has proven its effectiveness in sequence generation tasks, offers a promising avenue for addressing the fog application placement problem. By using a Seq2Seq model, we can capture the complex and non-linear relationships between application requirements, network conditions, and fog node capabilities. The model can learn patterns from historical placement data, enabling it to generate optimized placement decisions for new application instances.

However, to be truly effective and practical in fog computing scenarios, a Seq2Seq model needs to exhibit robust generalization capabilities. Generalization is crucial because fog environments are dynamic and inherently diverse, with application instances varying in their resource demands, communication patterns, and requirements. A generalized Seq2Seq model should be able to make accurate placement decisions for unseen application scenarios, including different application types, varying resource constraints, and dynamic network conditions.

Without a generalized Seq2Seq model, the application placement decisions may suffer from overfitting or underfitting. Overfitting may occur when the model memorizes the training data and fails to generalize well to unseen scenarios, resulting in poor performance and inefficient resource usage. Conversely, underfitting may lead to inadequate resource allocation and suboptimal placement decisions.

Therefore, exploring the generalization of Seq2Seq models for the fog application placement problem is of paramount importance. By enhancing the model's ability to handle diverse and dynamic fog environments, we can achieve more efficient, adaptive, and optimized application placement, ultimately leading to improved user experience, reduced latency, and enhanced overall system performance in fog computing scenarios. A generalized Seq2Seq model can be a valuable tool in meeting the challenges posed by the fog application placement problem and advancing the state-of-the-art in fog computing research and applications.

\section{Background}

\subsection{Achieving Generalized Fog Computing Application Placement Seq2seq Model}

Since Seq2Seq models train data in the form of a sequence, there are demerits when performing generalizations towards fog computing problems due to high heterogenity. The model are limited to generate output in a fixed length equal to the input. Input and output size is defined by the dataset length. Hence, if the input length is exceed the dataset defined length, an error would occur. 

Moreover, in the fog computing application placement problem, the dataset is mostly integer type instead of string type. This cause a different dot product produce by the model. The embedding output from the data wouldn't be the same as machine translation, or any other natural language task. This raised a concern for the fog problem to achieve generalization.

Therefore, to achieve generalization model has to be able to adapt with n-length input as there is no specific module numbers that application can have. Model also need to adapt with varying numbers that represents requirement. Since training all possible input wouldn't be the practical, optimizing the input sequence in inference inside the Seq2Seq architecture needs to be done.

\subsection{Generalization in Seq2Seq}

Generalization in the context of pre-processing for Seq2Seq models refers to the ability to apply a set of data preparation techniques that can be universally effective across different tasks, domains, and datasets. A generalized pre-processing pipeline ensures that the input data is appropriately formatted, transformed, and prepared to facilitate optimal learning by the Seq2Seq model, regardless of the specific task or dataset at hand. The goal is to create a versatile and robust pre-processing approach that can be easily adapted to various sequence generation tasks, thus reducing the need for task-specific data pre-processing strategies.


Fixed length of input encoder and decoder is needed for Attention Mechanism to calculate the Attention Weight. The Attention Mechanism will consider the sequence output based on every sequence index before to understand the context. Therefore, the output predicted was based on the embedding size of the input in form of dot product. However, if the input doesn't recognize by the model, the Attention Mechanism may not be able to handle this problem, and will only produce an error in the prediction.


Here are some generalization methods and principles for pre-processing in Seq2Seq models:

    Tokenization and Vocabulary Construction: A generalized pre-processing step involves tokenizing the input sequences into meaningful units, such as words, subwords, or characters. The vocabulary construction should be based on a diverse and representative corpus to handle a wide range of words and phrases in different languages or domains.

    Padding and Truncation: Sequences in the input data may vary in length, and Seq2Seq models require fixed-length input. A generalized approach should handle variable-length sequences by either padding shorter sequences or truncating longer ones, ensuring uniformity in sequence length.

    Data Cleaning and Normalization: Pre-processing should include data cleaning to remove irrelevant or noisy elements and normalization to standardize the data, such as converting text to lowercase, removing special characters, or normalizing numerical values.

    Handling Out-of-Vocabulary (OOV) Tokens: A robust pre-processing pipeline should handle OOV tokens encountered during inference, especially when dealing with unseen words or phrases. This could involve using subword tokenization or handling unknown tokens with special markers.

    Embedding and Feature Representation: Generalized pre-processing should facilitate the creation of word embeddings or other feature representations that capture semantic meaning and relationships between words effectively. This ensures that the model can extract meaningful information from the input sequences.

    Data Augmentation: Data augmentation, as part of pre-processing, can enhance generalization by artificially increasing the diversity of the training data. Techniques such as paraphrasing, back-translation, or adding noise to the input sequences can be applied in a generic manner.

    Handling Imbalanced Data: If the dataset exhibits class imbalances, a generalized pre-processing approach should include methods to address this issue, such as oversampling, undersampling, or class-weighted loss functions.

    Special Handling for Sequential Data: In some cases, sequential data may require additional pre-processing, such as time series alignment or handling temporal gaps.

    Language and Domain Adaptation: To ensure generalization across different languages or domains, pre-processing techniques should be designed to adapt to linguistic variations and domain-specific characteristics.

    Data Format Compatibility: A generalized pre-processing pipeline should handle various data formats commonly encountered in sequence generation tasks, such as text data, time series data, or audio data.

By incorporating these generalization methods into the pre-processing pipeline, Seq2Seq models can be more versatile and capable of handling diverse data sources, tasks, and domains. This helps reduce the effort required for task-specific data preparation and promotes the transferability of Seq2Seq models across different applications.


\section{Methodology}

\subsection{Pre-processing}

The Seq2Seq model will train pairs of input and output in the form of a sequence. The pre-processing method should be done to fit the dataset into the model. The input from the dataset will be considered as vocabulary and the whole input and output will be considered as pairs. The data consist of numbers, and using only one letter to tag the nodes, pre-processing is only done by splitting each input sequence into a one-dimensional array. Filtering the input pairs is done to check whether the sequence is equal to the determined length size. If the sequence length exceeds the determined length, the process will stop.

To assist the decoder during sequence generation, SOS (Start of Sentence), and EOS (End of Sentence) tokens will be used. SOS will be used for the start of the sequence when the model is needed to produce a sequence, while EOS will be used at the end of the sequence. There's also a PAD token to handle sequence to be equal during the batching phase. The PAD token will fill the sequence if the length of the sequence is not equal. UNK token will be used to handle input outside the known data that have been recognized by the model.

The UNK token is also used to tackle the generalization problem inside the model. Although the whole topology in the form of a sequence was trained, the model needs to understand the unknown input when testing the model. The UNK token represents the redefined index inside the model and replaces it with the token itself. Hence, the unknown sequence input will only be taken for the index that has been known by the model for output generation.

% minta tolong ini dijelasin metode tokenisasi yang kita pakai. dan proses preprosesing relevan lainnya

% jangan kebanyakan di model

\subsection{Experiment Setup}


\section{Results}

Our investigation into the generalization capabilities of deep learning models in the context of fog application placement yielded significant findings. The analysis was based on key parameters - average response time, byte size, and hop count - across various deep learning methods. The objective was to achieve the lowest values for these parameters, as lower values indicate more efficient and effective application placement in the fog network.

The 'multihead50000-16' method demonstrated an impressive performance, achieving lower average response times and byte sizes compared to other methods. This suggests that this method may offer efficient application placement with quicker response times and less data transmission, making it a promising candidate for implementation in real-world fog networks.

The 'naps' method showed a lower average hop count, indicating that it might provide more direct paths for data transmission. The fewer the number of hops, the less the potential for latency, suggesting that the 'naps' method could be effective in scenarios where minimizing latency is critical.

The 'multihead8' and 'multihead16' methods also demonstrated competitive performances. Although they did not achieve the lowest averages, their performance was close to the best-performing methods, showing their potential for effective application placement in fog networks.

The 'multiheadParmhead-8' method was excluded from our analysis due to its significantly lower performance compared to the other methods.

In our investigation into the generalization capabilities of deep learning models for fog application placement, we examined the average number of unique apps per method. This served as a measure of availability, a critical factor in ensuring the reliable and efficient operation of fog networks.

Availability, in this context, pertains to the capability of the placement algorithm to accommodate a diverse range of applications. A higher average number of unique apps indicates that the method can generalize well to various applications, demonstrating its adaptability and robustness in the face of diverse application requirements.

The 'naps' method exhibited the highest average number of unique apps, signifying superior availability. This suggests that the 'naps' method has impressive generalization capabilities, making it a versatile choice for application placement in a variety of fog network scenarios.

The 'multihead16' and 'multihead8' methods also demonstrated high availability, with a significant average number of unique apps. This indicates that these methods can effectively generalize to a wide range of applications, affirming their potential for diverse fog network environments.

On the other hand, the 'multihead50000-16' and 'multihead50000-8' methods showed slightly lower availability. While these methods still achieved respectable averages, their relatively lower scores suggest that their generalization capabilities might be more limited compared to the 'naps', 'multihead16', and 'multihead8' methods.


\section{Conclusion and Future Research}

Our research into the generalization capabilities of deep learning models for fog application placement yielded compelling insights. We found that these models could indeed adapt effectively to unseen scenarios, a crucial attribute for dynamic fog computing environments.

Methods such as 'multihead50000-16' and 'naps' distinguished themselves by achieving the lowest averages for key parameters like response time, byte size, and hop count. These lower averages signify efficient performance and robustness, thereby marking these methods as potential contenders for application placement in future fog networks.

Moreover, we identified availability, measured by the average number of unique apps per method, as a critical metric for assessing generalization. High availability indicates a method's adaptability and its ability to handle a diverse range of applications, thereby enhancing its resilience in varied fog network scenarios. In this regard, the 'naps', 'multihead16', and 'multihead8' methods showcased impressive availability, underlining their potential for efficient and versatile application placement.

However, the selection of a method should not solely hinge on these results but should also take into account the specific requirements of the network and the application. Each fog computing environment is unique and demands a tailored approach.

In conclusion, while our research sheds light on the promising generalization capabilities of several deep learning methods, it also emphasizes the need for further research. Refining these methods and enhancing their adaptability and efficiency is a crucial next step for harnessing the full potential of fog computing in the IoT era. These efforts will pave the way for more resilient, efficient, and intelligent fog networks, capable of meeting the challenges of our rapidly evolving digital landscape.

\subsection{Abbreviations and Acronyms}\label{AA}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have been defined in the abstract. Abbreviations such as 
IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
abbreviations in the title or heads unless they are unavoidable.

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
% An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page. \cite{aazam_offloading_2018}


\bibliographystyle{IEEEtran}
\bibliography{050723}


% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
